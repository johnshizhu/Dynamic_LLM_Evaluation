{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johns\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:151: UserWarning: Field \"model_max_budget\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from file_format import *\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = True\n",
    "mac = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_conversations = 10\n",
    "domain = \"Single Online Medical Diagnosis\"\n",
    "trait = \"Fairness\"\n",
    "trait_definition = \"The ethical principle of ensuring that LLMs are designed, trained, and deployed in ways that do not lead to biased or discriminatory outcomes and that they treat all users and groups equitably.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows: \n",
    "    gen_in_path = r'C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\gen_in.jsonl'\n",
    "if mac: \n",
    "    gen_in_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/gen_in.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_generation_file(num_conversations, domain, trait, trait_definition, gen_in_path, first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate first set of prompts\n",
    "\n",
    "if windows:\n",
    "    script_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\parallel_request.py\"\n",
    "    gen_in_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\gen_in.jsonl\"\n",
    "    gen_out_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\gen_out.jsonl\"\n",
    "\n",
    "if mac: \n",
    "    script_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/parallel_request.py\"\n",
    "    gen_in_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/gen_in.jsonl\"\n",
    "    gen_out_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/gen_out.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_args = subprocess.run(['python', script_path, gen_in_path, gen_out_path], capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows: \n",
    "    ver_output = build_verification_file(num_conversations, domain, trait, trait_definition, r'C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\gen_out.jsonl', r'C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\ver_in.jsonl', first=True)\n",
    "if mac:\n",
    "    ver_output = build_verification_file(num_conversations, domain, trait, trait_definition, '/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/gen_out.jsonl', '/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/ver_in.jsonl', first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    ver_in_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\ver_in.jsonl\"\n",
    "    ver_out_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\ver_out.jsonl\"\n",
    "\n",
    "if mac: \n",
    "    ver_in_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/ver_in.jsonl\"\n",
    "    ver_out_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/ver_out.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_args = subprocess.run(['python', script_path, ver_in_path, ver_out_path], capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    ver_out_file_path = r'C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\ver_out.jsonl'\n",
    "    gen_out_file_path = r'C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\gen_out.jsonl'\n",
    "    regen_in_file_path = r'C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\regen_in.jsonl'\n",
    "\n",
    "if mac: \n",
    "    ver_out_file_path = '/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/ver_out.jsonl'\n",
    "    gen_out_file_path = '/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/gen_out.jsonl'\n",
    "    regen_in_file_path = '/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/regen_in.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regen_output = build_regeneration_file(num_conversations, domain, trait, trait_definition, ver_out_file_path, gen_out_file_path, regen_in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    regen_in_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\regen_in.jsonl\"\n",
    "    regen_out_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\regen_out.jsonl\"\n",
    "\n",
    "if mac: \n",
    "    regen_in_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/regen_in.jsonl\"\n",
    "    regen_out_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/regen_out.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_args = subprocess.run(['python', script_path, regen_in_path, regen_out_path], capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "status = check_regen_results(num_conversations, regen_out_path)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    tar_in_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\tar_in.jsonl\"\n",
    "    tar_out_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\tar_out.jsonl\"\n",
    "if mac:\n",
    "    tar_in_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/tar_in.jsonl\"\n",
    "    tar_out_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/tar_out.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar_in emptied\n"
     ]
    }
   ],
   "source": [
    "# empty tar_in file\n",
    "prep_target_file(tar_in_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_in_res = build_target_file(num_conversations, tar_in_path, gen_out_file_path, regen_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call target model\n",
    "result_with_args = subprocess.run(['python', script_path, tar_in_path, tar_out_path], capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    history_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\history.json\"\n",
    "    tar_in_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\tar_in.jsonl\"\n",
    "    tar_out_path = r\"C:\\Users\\johns\\OneDrive\\Desktop\\LLM_Trust_Trust_Evaluation\\llmeval\\parallel\\tar_out.jsonl\"\n",
    "\n",
    "if mac:\n",
    "    history_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/history.json\"\n",
    "    tar_in_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/tar_in.jsonl\"\n",
    "    tar_out_path = \"/Users/john/Desktop/LLM_Trust_Trust_Evaluation/llmeval/parallel/tar_out.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prompt and response history\n",
    "res = save_history(num_conversations, 1, history_path, tar_in_path, tar_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
